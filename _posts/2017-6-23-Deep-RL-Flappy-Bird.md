---
layout: post
title: Deep Reinforcement Learning to play Flappy Bird using A3C algorithm
---
{:refdef: style="text-align: center;"}
![](images/animation.gif)
{: refdef}

# Overview
This project uses Asynchronous advantage actor-critic algorithm (A3C) to play Flappy Bird using Keras. The details of this algorithm are mentioned in [this paper](https://arxiv.org/pdf/1602.01783.pdf) by Deep Mind. The code for this project can be found in [this github repository.](https://github.com/shalabhsingh/A3C_Keras_FlappyBird)

# Installation Dependencies
* Python 3.5
* Keras 2.0
* pygame 
* scikit-image

# How to Run?
Clone the repository or download it. To test the pretrained model, run the {% highlight python %}test.py{% endhighlight %} file. To retrain the model from scratch, run the {% highlight python %}train_network.py{% endhighlight %} script. 

The trained models at different stages will be saved in "saved_models" folder.

# What is Deep Reinforcement Learning?
Deep Reinforcement learning is a technique that uses Deep Neural Networks to solve Reinforcement learning problems. Reinforcement learning is a machine learning method that lies somewhere between supervised and unsupervised learning. Whereas in supervised learning one has a target label for each training example and in unsupervised learning one has no labels at all, in reinforcement learning one has sparse and time-delayed labels – the rewards. Based only on those rewards the agent (bird in this problem) has to learn to behave in the environment (flappy bird game).

The following post is a must read for good introduction to Deep Reinforcement Learning - [Demystifying Deep Reinforcement Learning](https://www.nervanasys.com/demystifying-deep-reinforcement-learning/)

# Why A3C ?
Till 2016, Deep Q-learning was the go to method for solving reinforcement learning problems. Deep Q-learning was proposed by DeepMind back in 2013. Deep Q-learning was the first RL algorithm which was able to play games successfully and it became so popular that Google bought DeepMind itself. In february 2016, Google DeepMind proposed another set of algorithms for playing atari games, which were better than Deep Q-learning. The most successful of those algorithms is known as A3C which refers to Asynchronous advantage actor-critic algorithm for deep reinforcement learning. The advantages that A3C had over Deep Q learning are the following-

* Deep Q learning had a very large training time (~1 week on a GPU) whereas basic A3C takes 1 day to train on a CPU. (training time for Flappy Bird game in this project is barely 6 hours on a CPU !!)
* Deep Q learning used experience replay for getting good convergence which requires a lot of memory. A3C use multiple threads for this purpose which eliminates huge memory requirement.
* Deep Q learning is an off-policy learning algorithm that can update from data generated by an older policy (stored in experience replay), even though a better policy may have been discovered later. On the other hand, A3C is an on-policy method in which updates are made from data generated from current policy only.

However because of better exploration by DQN, it generally settles at global minima whereas A3C might settle at a local minima leading to sub-optimal results. 

A complete blog post can be written on Q-learning itself so as to explain technical terms like experience replay etc., however I won't delve into those topics. For knowing more about Deep Q learning and using it to play Flappy bird, see this blog post by Ben Lau - [Using Keras and Deep Q-Network to Play FlappyBird](https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html).


///////
**Learning Resources-**

1. For theoretical and implementation details of how a DQN works, see this blog page- https://yanpanlau.github.io/2016/07/10/FlappyBird-Keras.html
2. For theoretical and implementation details of how an A3C works, see this blog page- https://jaromiru.com/2017/03/26/lets-make-an-a3c-implementation/
///////////

# Asynchronous algorithms for Deep Reinforcement Learning
The new approach to solve Reinforcement Learning problems involve the use of asynchronous variants of standard Reinforcement Learning algorithms. Instead of experience replay, execute multiple agents are executed in parallel asynchronously, on multiple instances of the environment. This parallelism also decorrelates the agents’ data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states. This simple idea enables a much larger spectrum of fundamental on-policy RL algorithms, such as Sarsa, n-step methods, and actorcritic methods, as well as off-policy RL algorithms such as Q-learning, to be applied robustly and effectively using deep neural networks. The different algorithms used are as follows-

1. **Asynchronous one step Q-learning** - This is an off policy learning algorithm as the updates it makes are independent of the actions taken, as the we assume that maximum Q value action will be taken in the next frame. It is exactly similar to standard Deep Q learning, other than the fact that it uses asynchronous parallel agents rather that experience replay for getting statiscally independent inputs for making updated to the neural network.

/////
The algorithm performs actions based on \epsilon greedy policy (a random action is chosen with a probability \epsilon and an action predicted by the neural net is chosen with probability 1- \epsilon). the rewards obtained in that frame are measured and error is taken as

E[r + gamma* max_a ( Q(s',a') ) - Q(s, a) ]. A large chunk of such frames and actions are stored in an experience replay store, from where a random mini-batch of experiences are sampled each time after some time to update weights. However over here asynchronous methods instead of experience replay have been used. The epsilon value keeps decreasing gradually.
/////

2. **Asynchronous one-step SARSA** - It is same as Asynchronous one step Q learning except the fact that it uses an on policy approach to perform updates. In general one step SARSA leads to better performance that one step Q learning but it may well depend on case to case.

3. **Asyncronous n-Step Q-learning** - It is same as 1 step Q learning except the fact that it uses upto n steps in future to calculate the expected reward in the present step because of which updates in policy are transferred back to earlier states quickly.

4. **Asynchronous advantage actor-critic** - Asynchronous advantage actor-critic (A3C) is the fastest algorithm to train, among all. It has 2 versions, one that uses a Feed Forward Fully connected layer in the end of the network and another one that uses a LSTM (Long Short term memory) layer. This uses a policy based approach along with a critic which promotes or criticises any action taken, depending upon how much the actual reward is in comparison to the average reward it expected out of that state.

///
The update is grad( sum(Advantage * log p(a|theta)) ) + beta * sum(entropy) where advantage is actual reward - expected reward. the secnd term is the regularization term which prevents premature convergence. The policy and critic, both use the same network with 2 different outputs and respective losses. Optimizer used in all cases is RMSprop with initial learning rate of 7e-4 which is annealed to 0 gradually.
////

# Code Explanation

Now let us go through the code line by line. The code simply does the following:

1. The code receives the Game Screen Input in the form of a pixel array
2. The code does some image pre-processing to convert the image into required format for feeding to the neural network
3. The processed image will be fed into the neural network (Convolution Neural Network), and the network will then decide the best action to execute (Flap or Not Flap)

The network will be trained for a lot of times, to maximize the future expected reward.

### Game Screen Input

First of all, the FlappyBird is already written in Python via pygame, so here is the code snippet to access the FlappyBird API

{% highlight python %}
import wrapped_flappy_bird as game
x_t, r_t, terminal = game_state[thread_id].frame_step(a_t)
{% endhighlight %}

The idea is quite simple, the input is **a_t** (0 represent don't flap, 1 represent flap), the API will give you the next frame **x_t1_colored**, the **reward** (0.1 if alive, +1 if pass the pipe, -1 if die) and **terminal** is a boolean flag indicates whether the game is FINISHED or NOT. As there are multiple threads in which the game is running, **thread_id** denotes the thread of the running game.

Interesting readers can modify the reward function in **game/wrapped_flappy_bird.py", under the function **def frame_step(self, input_actions)**

### Image pre-processing

{:refdef: style="text-align: center;"}
![](/images/bird.jpg)
{: refdef}

In order to make the code train faster, it is vital to do some image processing. Here are the key elements:

1. Convert the color image into grayscale.
2. Crop down the image size into 84x84 pixel as suggessted by the paper.
3. Stack 4 frames together before feeding into neural network. 

Why is it needed to stack 4 frames together? This is one way for the model to be able to infer the velocity information of the bird.

{% highlight python %}
def preprocess(image):
	image = skimage.color.rgb2gray(image)
	image = skimage.transform.resize(image, (IMAGE_ROWS,IMAGE_COLS), mode = 'constant')	
	image = skimage.exposure.rescale_intensity(image, out_range=(0,255))
	image = image.reshape(1, image.shape[0], image.shape[1], 1)
	return image
{% endhighlight %}

The input to the preprocess function is a single frame **image** , which is resized to the size 84x84 (as IMAGE_ROWS = IMAGE_COLS = 84). Later on, 4 such images are stacked onto each other and then given as input to the neural network.

## Model Definition
Now as we have processed the input into the model, we can describe the model itself now-

{% highlight python %}
def buildmodel():
	print("Model buliding begins")

	model = Sequential()
	keras.initializers.RandomUniform(minval=-0.1, maxval=0.1, seed=None)

	S = Input(shape = (IMAGE_ROWS, IMAGE_COLS, IMAGE_CHANNELS, ), name = 'Input')
	h0 = Convolution2D(16, kernel_size = (8,8), strides = (4,4), activation = 'relu', kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform')(S)
	h1 = Convolution2D(32, kernel_size = (4,4), strides = (2,2), activation = 'relu', kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform')(h0)
	h2 = Flatten()(h1)
	h3 = Dense(256, activation = 'relu', kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform') (h2)
	P = Dense(1, name = 'o_P', activation = 'sigmoid', kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform') (h3)
	V = Dense(1, name = 'o_V', kernel_initializer = 'random_uniform', bias_initializer = 'random_uniform') (h3)

	model = Model(inputs = S, outputs = [P,V])
	rms = RMSprop(lr = LEARNING_RATE, rho = 0.99, epsilon = 0.1)
	model.compile(loss = {'o_P': binarycrossentropy, 'o_V': sumofsquares}, loss_weights = {'o_P': 1., 'o_V' : 0.5}, optimizer = rms)
	return model
{% endhighlight %}

The exact architecture is following : The input to the neural network consists of an 80x80x4 images. The first hidden layer convolves 16 filters of 8x8 with stride 4 and applies ReLU activation function. The 2nd layer convolves 32 filters of 4 x 4 with stride 2 and applies ReLU activation function. All the layers in second layer are then put side by side (flattened) and a final hidden layer, which is fully-connected consisting of 256 ReLU units is added upon it. The output layer following the final hidden layer has two types of output-

1. First is the policy output containing one neuron for each possible action (flap or no flap). The value in each of these neurons indicates the probability of that particular action to be taken, for the given input. The sum of all the outputs of this type is hence 1. However in this game, as there are only 2 outputs, we will use only one output unit indicating the probability of flap. The probability of not flapping is then one minus the output unit value. Sigmoid activation is used for this output so as to restrict the output value in valid range of probabilities [0,1].

2. The second type of output is critic output. For each input to the neural network, the network also outputs the expected reward from these states. The expected reward value acts a feedback to the network. If an action results in a reward higher than that predicted by the critic, then the weights of the neural network are tweaked in a manner so that this action is promoted, the next time a similar state occurs. Similarly, actions yielding less reward than that predicted by the critic are less likely to occur in feature. This is the method that A3C uses to promote higher rewards actions in the future.

As the first category of outputs have range between 0 and 1, the loss used is categorical cross entropy. For the second category of outputs, the loss used is mean squared error (MSE). 

# Parallel Processing
Finally, now as our model is ready we are need to define parallel threads for updates. In this project, 16 parallel threads have been defined, each of which runs until terminal it true (bird died) or until $$ {t}_{max} $$  steps have been performed, before weight updates are backpropogated. The value of $$ {t}_{max} $$ used is 5 steps and hence the maximum number of inputs in each batch is 16x5 = 80.

{% highlight python %}
class actorthread(threading.Thread):
	def __init__(self,thread_id, s_t, FIRST_FRAME, t):
		threading.Thread.__init__(self)
		self.thread_id = thread_id
		self.s_t = s_t
		self.FIRST_FRAME = FIRST_FRAME
		self.t = t

	def run(self):
		global episode_output
		global episode_r
		global episode_critic
		global episode_state

		threadLock.acquire()
		self.t, state_store, output_store, r_store, critic_store, self.FIRST_FRAME = runprocess(self.thread_id, self.s_t, self.FIRST_FRAME, self.t)
		self.s_t = state_store[-1]
		self.s_t = self.s_t.reshape(1, self.s_t.shape[0], self.s_t.shape[1], self.s_t.shape[2])

		episode_r = np.append(episode_r, r_store)
		episode_output = np.append(episode_output, output_store)
		episode_state = np.append(episode_state, state_store, axis = 0)
		episode_critic = np.append(episode_critic, critic_store)

		threadLock.release()
{% endhighlight %}

A class actor thread is defined which maintains information about each thread. Each thread has a thread_id, and is initialized with an initial state s_t (s_t contains 4 stacked game frames) to be given as input to the neural network. FIRST_FRAME is a boolean variable which stores if the given state s_t is the first state of a new game, or is the state of an old game until which the game was played, before the last update. Function threadLock.acquire() is used to force the thread to run synchronously and threadLock.release() is used to release the lock when it is no longer required (all threads have finished execution).

Each thread comes to action when it calls runprocess function. The major components of runprocess function are mentioned below-

{% highlight python %}
def runprocess(thread_id, s_t, FIRST_FRAME, t):
	global T
	global a_t
	global model

	while t-t_start < t_max and terminal == False:
		t += 1
		T += 1
		intermediate_output = 0
		
		if FIRST_FRAME == False:
			with graph.as_default():
				out = model.predict(s_t)[0]			
				intermediate_output = intermediate_layer_model.predict(s_t)
			no = np.random.rand()
			a_t = [0,1] if no < out else [1,0]  #stochastic action
			#a_t = [0,1] if 0.5 <y[0] else [1,0]  #deterministic action

		x_t, r_t, terminal = game_state[thread_id].frame_step(a_t)
		x_t = preprocess(x_t)

		if FIRST_FRAME:
			s_t = np.concatenate((x_t, x_t, x_t, x_t), axis=3)
			FIRST_FRAME = False
		else:
			s_t = np.append(x_t, s_t[:, :, :, :3], axis=3)

		y = 0 if a_t[0] == 1 else 1
		
		with graph.as_default():
			critic_reward = model.predict(s_t)[1]

		r_store = np.append(r_store, r_t)
		state_store = np.append(state_store, s_t, axis = 0)
		output_store = np.append(output_store, y)
		
	if terminal == False:
		r_store[len(r_store)-1] = critic_store[len(r_store)-1]
	else:
		r_store[len(r_store)-1] = -1
		FIRST_FRAME = True
	
	for i in range(2,len(r_store)+1):
		r_store[len(r_store)-i] = r_store[len(r_store)-i] + GAMMA*r_store[len(r_store)-i + 1]

	return t, state_store, output_store, r_store, critic_store, FIRST_FRAME
{% endhighlight %}

The runprocess function, starts with defining the while loop in which each frame is processed. For each frame the action choosen is flap with a probability equal to the policy output (first output type). However if it is the first frame of a new game, the action choosen is not to flap, by default. This is done because we still cannot stack 4 consecutive frames in s_t to give as input to the network. Hence all the 4 frames in s_t are taken to be same as the first frame and default action of no flap is choosen as the network doesn't have any knowledge of bird movement from those frames.

The thread runs for a maximum of $$ {t}_{max} $$ steps and at each step, the state, action taken, reward obtained at each step and expected reward predicted by critic networks are stored in arrays- state_store, output_store, r_store and critic_store respectively. Later, these arrays for each thread are concatenated and then send to the model for training. The actual discounted reward value for each frame in the thread is calculated by rewards obtained in each step using the followin formula-

$$ 
r(s_t) = r(s_t) + \gammma * r(s_t') 
$$

where s_t' is the state succeeding the current state. However, the discounted reward value for the final step of a thread is taken to be the same as the reward predicted by the critic network. 

# Model Description
I hope that the model definition and thread configuration is clearly understood by now. Now we will discuss about the loss function and the hyperparameters used by the model. As per the suggesstion given by the paper the following loss function is used by the network-

$$ 
L_{policy} = \sigma log \pi(a_{t}|s_{t}; \theta A(s_{t}, a_{t};\theta , {\theta}_{v}
$$

The hyperparameters used are-
* Learning rate = 7e-4 which is decreased by 3.2e-8 (can be tuned better) every update.
* No. of threads = 16
* Frames/thread used for each update = 5
* Reward discount (gamma) = 0.99
* RMSProp cache decay rate = 0.99
* Entropy regularization, as suggested by the paper has not been used. However I believe that using it, could lead to better performance of the model.

The best model I have got is still not very good but is able to cross 5 pipes on average (i.e. it has developed a good understanding of when to flap and when not to). To train better models, tinkering with above hyperparameters can be beneficial.


# From Blog

Summary

This project was mainly focused on tuning the hyperparameters of the neural net as it is pretty difficult to converge as a3c searches for local optimum rather than global optimum. Another focus was to learn about the state of the art algorithms that are changing the deep learning scenarios, by producing universal AIs.

